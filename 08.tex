\section{
 Приближение матрицей указанного ранга и SVD-разложение...
}
\textbf{Приближение матрицей указанного ранга и SVD-разложение. Возможность применения к сжатию изображения.}
\textbf{Необработанная версия из конспекта Константина Михайловича}

\dfn[Сингулярные значения] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сингулярными значениями $A$ называются числа $\sigma_i=\sqrt{d_i}$, где $d_i>0$ -- положительные собственные числа оператора $A^*A \colon V \to V$. Если же говорить на языке матриц, то для матрицы $X$ её сингулярными значениями будут корни из собственных чисел $X^{\top}X$. 
\edfn

На самом деле мы не обсуждали определение сопряжённого линейного отображения, а только сопряжённого оператора. Напишу немного об этом.

\dfn Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда сопряжённым отображением к $A$, называется такое линейное отображение $A^{*}$, что $\lan A^*x,y\ran = \lan x,Ay\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы, то матрица сопряжённого отображения в этих базисах будет равна транспонированной матрице исходного.
\proof Достаточно доказать последнюю часть, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_j$ и $v_i$. Обозначим матрицу $A$ в этом базисе за $X$, а кандидата на $A^*$ за $Y$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполнения на базисных. Иными словами необходимо и достаточно, чтобы $\lan X^{*}e_i,e_j\ran=\lan e_i,Xe_j\ran$. Но первая часть даёт $X^{*}_{ji}$, а вторая -- $X_{ij}$. Итого необходимо и достаточно, чтобы $X^{*}=X^{\top}$.   
\endproof
\ethrm

Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $A$ -- линейное отображение $A\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $A$ имеет вид 
$$\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $A$, числа $\Sigma=\sigma_1, \dots, \sigma_r$ её сингулярные значения.
На языке матриц это означает, что для любой матрицы $X \in M_{m\times n}$ существуют матрицы $L$ -- размера $m$ и $R$ -- размера $n$,  что
$$X= L \Sigma R,$$
 с теми же условиями на $r$ и $\sigma_i$.
 
\proof Рассмотрим оператор $B = A^{*}A$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Ae_i \in U$. Они ортогональны. Действительно
$$\lan Ae_i, Ae_j\ran = \lan A^{*}Ae_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $||e_i||^2=d_i$. Возьмём 
$$f_i=\frac{Ae_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Ae_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $A$
Напоследок осталось решить вопрос, как выглядит матрица $R$. В нашей конструкции матрица $R$ есть матрица замены из стандартного базиса в базис из собственных векторов $e_i$ матрицы $X^{\top}X$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $R=C^{-1}$, но $C$ ортогональна и поэтому можно написать $R=C^{T}$, то есть строки $R$ -- собственные вектора $X^{\top}X$. Часто эти вектора называют правыми сингулярными векторами $X$.

\endproof
\ethrm

Наличие такого разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.

\zd Получите аналогичное описание для $L$. Покажите так же, что ничего кроме $\Sigma$ в аналогичном разложении получится не может.
\ezd

SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $L$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле вот такой вот матричной нормы, называемой, нормой Фробениуса 
$$||X||_F=\sqrt{\sum_{i,j} a_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом нахождение проекций точек можно переформулировать, как нахождение ближайшей к матрице $X$ матрице ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение

\thrm Пусть $X\in M_{m\times n}(\mb R)$. И SVD-разложение $X$ имеет вид $X=L\Sigma R$, где на диагонали $\Sigma$ стоят $\sigma_1,\dots,\sigma_r$ и нули. Тогда наилучшим приближением ранга $k$ в смысле нормы Фробениуса к матрице $X$ будет матрица $X^{(k)}=L\Sigma^{(k)}R$, где на диагонали $\Sigma^{(k)}$ стоят $\sigma_1,\dots,\sigma_{k}$ и нули.
\proof Для того, чтобы найти матрицу $X^{(k)}$ необходимо спроецировать строки $X$ на подпространство $L=\lan v_1^{\top},\dots,v_k^{\top}\ran$, где $v_i$ ортонормированный базис из собственных векторов $X^{\top}X$.  Вспомним, что строки $R$ есть $v_i^{\top}$. Для того, чтобы спроецировать одну строку $a$ на пространство $V^{(k)}$ необходимо вычислить сумму $\sum_{i=1}^k (av_i)v_i^{\top}$. Применив это целиком к матрице $X=L\Sigma R$ получим 
$$X^{(k)}=\sum_{i=1}^k Xv_iv_i^{\top}=L\Sigma \left(\sum_{i=1}^k Rv_iv_i^{\top}\right).$$
Вычислим последнюю сумму. Эта сумма считает проекции строк $R$ на $L$. Но первые $k$ строк лежат в $L$, а остальные ортогональны $L$. Итого имеем
$$R^{(k)}=\sum_{i=1}^k Rv_iv_i^{\top} = \pmat v_1^{\top} \\ \vdots \\ v_k^{\top} \\ 0 \\ \vdots \\ 0 \epmat.$$
Осталось заметить, что $\Sigma^{(k)} R= \Sigma^{(k)}R^{(k)}=\Sigma R^{(k)}$.
\endproof
\ethrm 

SVD-разложение используется в разных задачах, в том числе и для сжатия изображений.  Для простоты рассмотрим случай квадратного $n \times n$ чёрно белого изображения. Сделаем из него вещественную матрицу $X$ размера $n \times n$ и найдём SVD-разложение $L \Sigma K$. Тогда приближение $X^{(k)}$ задаётся $L\Sigma^{(k)}R$. Однако, как мы уже заметили, вместо матрицы $R$ можно взять матрицу $R^{(k)}$. Аналогично вместо $L$ можно взять $L^{(k)}$ -- выкинув из $L$ последние $n-k$ столбцов. Для хранения матрицы $\Sigma^{(k)}$ нужно $k$ параметров, для матриц $L^{(k)}$ и $R^{(k)}$ по $kn$ параметров. Итого нужно $2kn+k$ параметров. Однако чтобы не хранить отдельно $\Sigma$ её можно домножить на $L$ и хранить $L\Sigma$. В таком случае необходимо $2kn$ параметров. При $k<\frac{n}{2}$ это даёт эффект сжатия. 

Однако, это не предел. Посмотрим, сколько параметров нужно, чтобы задать $X$ -- матрицу ранга $k$. Пусть главный минор размера $k$ матрицы $X$ не ноль (априори мы знаем, что какой-то минор такого размера не ноль). Тогда для $j$-ой строки матрицы, начиная с номера $j \geq k+1$ есть набор чисел $a_{1,j},\dots,a_{k,j}$, которые есть коэффициенты в линейной комбинации дающей из первых строк $j$-ую. Аналогично для столбцов. Такой набор данных задаётся $k^2+ 2k(n-k)=2kn-k^2$ параметрами. Осталось заметить, что всегда $2kn - k^2\leq n^2$ так как $0\leq n^2-2kn+k^2=(n-k)^2$. Если невырожденным оказался не главный минор, то дополнительно нужно задать $2k$ дискретных параметров задающий номера строк и столбцов невырожденного минора.

