\section{
 Приближение матрицей указанного ранга и SVD-разложение...
}
\textbf{Приближение матрицей указанного ранга и SVD-разложение. Возможность применения к сжатию изображения.}

SVD-разложение используется в практическом решении задачи из метода главных компонент (билет 6) и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $L$, то получится матрица ранга $k$ или меньше ($k$ -- это размерность пространства, которое ищем). Эта матрица будет ближайшей к исходной в смысле вот такой вот матричной нормы, называемой, нормой Фробениуса 
$$||X||_F=\sqrt{\sum_{i,j} a_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$

Последнее равенство:

$$(X^{\top}X)_{jj} = \sum\limits_{i} X^{\top}_{ji}X_{ij} = \sum\limits_{i} X_{ij}X_{ij} = \sum\limits_{i} a_{ij}^2$$

$$\Tr X^{\top}X = \sum\limits_{j} (X^{\top}X)_{jj} = \sum\limits_{i,j} a_{ij}^2$$

Таким образом нахождение проекций точек можно переформулировать как нахождение ближайшей к матрице $X$ матрицы ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение.

\thrm Пусть $X\in M_{m\times n}(\mb R)$. И SVD-разложение $X$ имеет вид $X=L\Sigma R$, где на диагонали $\Sigma$ стоят $\sigma_1,\dots,\sigma_r$ и нули. Тогда наилучшим приближением ранга $k$ в смысле нормы Фробениуса к матрице $X$ будет матрица $X^{(k)}=L\Sigma^{(k)}R$, где на диагонали $\Sigma^{(k)}$ стоят $\sigma_1,\dots,\sigma_{k}$ и нули.
\proof Для того, чтобы найти матрицу $X^{(k)}$ необходимо спроецировать строки $X$ на подпространство $L_k=\lan v_1^{\top},\dots,v_k^{\top}\ran$, где $v_i$ ортонормированный базис из собственных векторов $X^{\top}X$.  Вспомним, что строки $R$ есть $v_i^{\top}$.

Для того, чтобы спроецировать одну строку $a$ на пространство $V^{(k)}$ необходимо вычислить сумму $\sum_{i=1}^k (av_i) v_i^{\top}$. $av_i = \lan a^{\top}, v_i \ran$ -- это как раз проекция $a$ на $v_i$ (коэффициент).

Применив это целиком к матрице $X=L\Sigma R$, получим 
$$X^{(k)}=\sum_{i=1}^k Xv_iv_i^{\top}=\sum_{i=1}^k L\Sigma Rv_iv_i^{\top}=L\Sigma \left(\sum_{i=1}^k Rv_iv_i^{\top}\right).$$
Вычислим последнюю сумму. Эта сумма считает проекции строк $R$ на $L_k$. Но первые $k$ строк лежат в $L_k$, а остальные ортогональны $L_k$. Итого имеем
$$R^{(k)}=\sum_{i=1}^k Rv_iv_i^{\top} = \pmat v_1^{\top} \\ \vdots \\ v_k^{\top} \\ 0 \\ \vdots \\ 0 \epmat.$$
Осталось заметить, что $\Sigma^{(k)} R= \Sigma^{(k)}R^{(k)}=\Sigma R^{(k)}$ (в произведении дальше $k$-ой строки или $k$-ого столбца в любом случае все 0).
\endproof
\ethrm 

SVD-разложение используется в разных задачах, в том числе и для сжатия изображений.  Для простоты рассмотрим случай квадратного $n \times n$ чёрно-белого изображения. Сделаем из него вещественную матрицу $X$ размера $n \times n$ и найдём SVD-разложение $L \Sigma K$. Тогда приближение $X^{(k)}$ задаётся $L\Sigma^{(k)}R$. Однако, как мы уже заметили, вместо матрицы $R$ можно взять матрицу $R^{(k)}$. Аналогично вместо $L$ можно взять $L^{(k)}$ -- выкинув из $L$ последние $n-k$ столбцов. Для хранения матрицы $\Sigma^{(k)}$ нужно $k$ параметров (она диагональная), для матриц $L^{(k)}$ и $R^{(k)}$ по $kn$ параметров. Итого нужно $2kn+k$ параметров. Однако чтобы не хранить отдельно $\Sigma$ её можно домножить на $L$ и хранить $L\Sigma$. В таком случае необходимо $2kn$ параметров. При $k<\frac{n}{2}$ это даёт эффект сжатия. 

Однако, это не предел. Посмотрим, сколько параметров нужно, чтобы задать $X$ -- матрицу ранга $k$. У нее есть невырожденная квадратная подматрица размера $k$. Пусть это просто первые $k$ строк и столбцов. Тогда для $j$-ой строки матрицы, начиная с номера $j \geq k+1$ есть набор чисел $a_{1,j},\dots,a_{k,j}$, которые есть коэффициенты в линейной комбинации, дающей из первых $k$ строк $j$-ую (набор из первых $k$ строк и $j$-ой строки линейно зависим). Аналогично для столбцов. Такой набор данных задаётся $k^2+ 2k(n-k)=2kn-k^2$ параметрами ($k^2$ коэффициентов квадратной подматрицы, по $k$ коэффициентов линейной комбинации для $n - k$ строк и $n - k$ столбцов). Осталось заметить, что всегда $2kn - k^2\leq n^2$ так как $0\leq n^2-2kn+k^2=(n-k)^2$. Если невырожденным оказался не главный минор, то дополнительно нужно задать $2k$ дискретных параметров, задающих номера строк и столбцов невырожденного минора.

$$\lan Ae_i, Ae_j\ran = \lan A^{*}Ae_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $||e_i||^2=d_i$. Возьмём 
$$f_i=\frac{Ae_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Ae_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $A$
Напоследок осталось решить вопрос, как выглядит матрица $R$. В нашей конструкции матрица $R$ есть матрица замены из стандартного базиса в базис из собственных векторов $e_i$ матрицы $X^{\top}X$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $R=C^{-1}$, но $C$ ортогональна и поэтому можно написать $R=C^{T}$, то есть строки $R$ -- собственные вектора $X^{\top}X$. Часто эти вектора называют правыми сингулярными векторами $X$.

\endproof
\ethrm

Наличие такого разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.

\zd Получите аналогичное описание для $L$. Покажите так же, что ничего кроме $\Sigma$ в аналогичном разложении получится не может.
\ezd

SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $L$, то получится матрица ранга $k$ или меньше. Эта матрица будет ближайшей к исходной в смысле вот такой вот матричной нормы, называемой, нормой Фробениуса 
$$||X||_F=\sqrt{\sum_{i,j} a_{ij}^2}=\sqrt{\Tr X^{\top}X}.$$
Таким образом нахождение проекций точек можно переформулировать, как нахождение ближайшей к матрице $X$ матрице ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение

\thrm Пусть $X\in M_{m\times n}(\mb R)$. И SVD-разложение $X$ имеет вид $X=L\Sigma R$, где на диагонали $\Sigma$ стоят $\sigma_1,\dots,\sigma_r$ и нули. Тогда наилучшим приближением ранга $k$ в смысле нормы Фробениуса к матрице $X$ будет матрица $X^{(k)}=L\Sigma^{(k)}R$, где на диагонали $\Sigma^{(k)}$ стоят $\sigma_1,\dots,\sigma_{k}$ и нули.
\proof Для того, чтобы найти матрицу $X^{(k)}$ необходимо спроецировать строки $X$ на подпространство $L=\lan v_1^{\top},\dots,v_k^{\top}\ran$, где $v_i$ ортонормированный базис из собственных векторов $X^{\top}X$.  Вспомним, что строки $R$ есть $v_i^{\top}$. Для того, чтобы спроецировать одну строку $a$ на пространство $V^{(k)}$ необходимо вычислить сумму $\sum_{i=1}^k (av_i)v_i^{\top}$. Применив это целиком к матрице $X=L\Sigma R$ получим 
$$X^{(k)}=\sum_{i=1}^k Xv_iv_i^{\top}=L\Sigma \left(\sum_{i=1}^k Rv_iv_i^{\top}\right).$$
Вычислим последнюю сумму. Эта сумма считает проекции строк $R$ на $L$. Но первые $k$ строк лежат в $L$, а остальные ортогональны $L$. Итого имеем
$$R^{(k)}=\sum_{i=1}^k Rv_iv_i^{\top} = \pmat v_1^{\top} \\ \vdots \\ v_k^{\top} \\ 0 \\ \vdots \\ 0 \epmat.$$
Осталось заметить, что $\Sigma^{(k)} R= \Sigma^{(k)}R^{(k)}=\Sigma R^{(k)}$.
\endproof
\ethrm 

SVD-разложение используется в разных задачах, в том числе и для сжатия изображений.  Для простоты рассмотрим случай квадратного $n \times n$ чёрно белого изображения. Сделаем из него вещественную матрицу $X$ размера $n \times n$ и найдём SVD-разложение $L \Sigma K$. Тогда приближение $X^{(k)}$ задаётся $L\Sigma^{(k)}R$. Однако, как мы уже заметили, вместо матрицы $R$ можно взять матрицу $R^{(k)}$. Аналогично вместо $L$ можно взять $L^{(k)}$ -- выкинув из $L$ последние $n-k$ столбцов. Для хранения матрицы $\Sigma^{(k)}$ нужно $k$ параметров, для матриц $L^{(k)}$ и $R^{(k)}$ по $kn$ параметров. Итого нужно $2kn+k$ параметров. Однако чтобы не хранить отдельно $\Sigma$ её можно домножить на $L$ и хранить $L\Sigma$. В таком случае необходимо $2kn$ параметров. При $k<\frac{n}{2}$ это даёт эффект сжатия. 